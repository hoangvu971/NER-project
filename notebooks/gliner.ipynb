{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 39016.78it/s]\n",
      "/opt/conda/envs/training-pipeline/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../models/gliner_medium-v2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_SAVE_PATH = \"../models/gliner_medium-v2.1/model.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "text = \"ONNX is an open-source format designed to enable the interoperability of AI models across various frameworks and tools.\"\n",
    "labels = ['format', 'model', 'tool', 'cat']\n",
    "\n",
    "inputs, _ = model.prepare_model_inputs([text], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W shape_type_inference.cpp:1974] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1974] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1974] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if model.config.span_mode == 'token_level':\n",
    "    all_inputs =  (inputs['input_ids'], inputs['attention_mask'], \n",
    "                    inputs['words_mask'], inputs['text_lengths'])\n",
    "    input_names = ['input_ids', 'attention_mask', 'words_mask', 'text_lengths']\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"words_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"text_lengths\": {0: \"batch_size\", 1: \"value\"},\n",
    "        \"logits\": {0: \"position\", 1: \"batch_size\", 2: \"sequence_length\", 3: \"num_classes\"},\n",
    "    }\n",
    "else:\n",
    "    all_inputs =  (inputs['input_ids'], inputs['attention_mask'], \n",
    "                    inputs['words_mask'], inputs['text_lengths'],\n",
    "                    inputs['span_idx'], inputs['span_mask'])\n",
    "    input_names = ['input_ids', 'attention_mask', 'words_mask', 'text_lengths', 'span_idx', 'span_mask']\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"words_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"text_lengths\": {0: \"batch_size\", 1: \"value\"},\n",
    "        \"span_idx\": {0: \"batch_size\", 1: \"num_spans\", 2: \"idx\"},\n",
    "        \"span_mask\": {0: \"batch_size\", 1: \"num_spans\"},\n",
    "        \"logits\": {0: \"batch_size\", 1: \"sequence_length\", 2: \"num_spans\", 3: \"num_classes\"},\n",
    "    }\n",
    "print('Converting the model...')\n",
    "torch.onnx.export(\n",
    "    model.model,\n",
    "    all_inputs,\n",
    "    f=ONNX_SAVE_PATH,\n",
    "    input_names=input_names,\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=14,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/NonZero_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"unk__788\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/NonZero_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"unk__789\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/NonZero_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"unk__803\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#quantize model\n",
    "import os\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantized_save_path = (\"../models/gliner_medium-v2.1/model_quantized.onnx\")\n",
    "# Quantize the ONNX model\n",
    "print(\"Quantizing the model...\")\n",
    "quantize_dynamic(\n",
    "    ONNX_SAVE_PATH,  # Input model\n",
    "    quantized_save_path,  # Output model\n",
    "    weight_type=QuantType.QUInt8  # Quantize weights to 8-bit integers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in /workspaces/NER-project/models/gliner_medium-v2.1\n",
      "WARNING:huggingface_hub.hub_mixin:config.json not found in /workspaces/NER-project/models/gliner_medium-v2.1\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "model = GLiNER.from_pretrained(\"../models/gliner_medium-v2.1\", load_onnx=True, load_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "text2 = \"My name is Tom, I live in New York and my girlfriend's name is Elaine. Our parents live in Viet Nam, Nha Trang city, and their names are Que and Mai\"\n",
    "labels = ['Person', 'Place']\n",
    "\n",
    "inputs, raw_batch = model.prepare_model_inputs([text2], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "ort_sess = ort.InferenceSession('../models/gliner_medium-v2.1/model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "outputs = ort_sess.run(None, {'input_ids': inputs['input_ids'].numpy(),\n",
    "                            'attention_mask': inputs['attention_mask'].numpy(),\n",
    "                            'words_mask': inputs['words_mask'].numpy(),\n",
    "                            'text_lengths': inputs['text_lengths'].numpy(),\n",
    "                            'span_idx': inputs['span_idx'].numpy(),\n",
    "                            'span_mask': inputs['span_mask'].numpy(),\n",
    "                            })[0]\n",
    "outputs = torch.from_numpy(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 3, 'Person', 0.9672063589096069),\n",
       " (8, 9, 'Place', 0.8985015153884888),\n",
       " (17, 17, 'Person', 0.9670381546020508),\n",
       " (23, 24, 'Place', 0.9371719360351562),\n",
       " (26, 28, 'Place', 0.9003996849060059),\n",
       " (34, 34, 'Person', 0.8820420503616333),\n",
       " (36, 36, 'Person', 0.7397370934486389)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.decoder.decode(\n",
    "            raw_batch[\"tokens\"],\n",
    "            raw_batch[\"id_to_classes\"],\n",
    "            outputs,\n",
    "            flat_ner=True,\n",
    "            threshold=0.5,\n",
    "            multi_label=False,\n",
    "        )[0]\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tom'] => Person\n",
      "['New', 'York'] => Place\n",
      "['Elaine'] => Person\n",
      "['Viet', 'Nam'] => Place\n",
      "['Nha', 'Trang', 'city'] => Place\n",
      "['Que'] => Person\n",
      "['Mai'] => Person\n"
     ]
    }
   ],
   "source": [
    "texts = raw_batch['tokens'][0]\n",
    "\n",
    "for output in outputs:\n",
    "    start, end = output[:2]\n",
    "    entity = output[2]\n",
    "    print(f\"{texts[start:end+1]} => {entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bentoml\n",
    "# import onnx \n",
    "\n",
    "# model_onnx = onnx.load(ONNX_SAVE_PATH)\n",
    "# signatures = {\n",
    "#     \"run\": {\"batchable\": True},\n",
    "# }\n",
    "# bento_model = bentoml.onnx.save_model(\"onnx_ner\", model_onnx, signatures=signatures)\n",
    "# print(bento_model.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
